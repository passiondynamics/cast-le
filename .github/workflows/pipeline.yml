name: cineguess-backend
run-name: ${{github.run_id}}.${{github.run_attempt}}
on:
  push:
    branches:
      - main
  pull_request:
    types:
      - opened
      - synchronize
      - reopened
      - assigned
      - unassigned
    branches:
      - main

# For authenticating with AWS.
permissions:
  id-token: write

jobs:
  init:
    name: Initialize
    runs-on: ubuntu-latest
    steps:
      # Because we can't derive env variables from other env variables...
      - name: Set up pipeline vars
        id: pipeline_vars
        run: |
          rn="${{github.event.repository.name}}"
          pi="${{github.event.repository.owner.login}}.$rn.${{github.run_id}}.${{github.run_attempt}}"
          af="artifact-$pi.zip"
          if [ "${{github.event_name}}" == "push" ]; then
              de="prod"
              gai="null"
          else
              de="dev"
              gai=$(jq -cn '["${{github.event.pull_request.user.id}}"] + [${{toJSON(github.event.pull_request.assignees)}}[].id | tostring] | unique')
          fi
          abk="$de/$rn/$af"

          echo "[*] Pipeline vars:"
          printf "artifact_filename=$af\nartifact_bucket_key=$abk\ndeployment_env=$de\npipeline_id=$pi\nrepository_name=$rn\ngithub_assignee_ids=$gai\n" | tee -a "$GITHUB_OUTPUT"
    outputs:
      artifact_filename: ${{steps.pipeline_vars.outputs.artifact_filename}}
      artifact_bucket_key: ${{steps.pipeline_vars.outputs.artifact_bucket_key}}
      deployment_env: ${{steps.pipeline_vars.outputs.deployment_env}}
      repository_name: ${{steps.pipeline_vars.outputs.repository_name}}
      pipeline_id: ${{steps.pipeline_vars.outputs.pipeline_id}}
      github_assignee_ids: ${{steps.pipeline_vars.outputs.github_assignee_ids}}
  build:
    name: Build
    needs: init
    runs-on: ubuntu-latest
    # Needed for access to vars defined in Github for `dev` and `prod`.
    environment: ${{needs.init.outputs.deployment_env}}
    env:
      # Use env vars for easier repeated reference.
      artifact_filename: ${{needs.init.outputs.artifact_filename}}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          sparse-checkout: |
            cft.yml
            Pipfile.lock
            src/
            tests/
          persist-credentials: false
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Set up pipenv
        run: |
          echo "[*] Installing pipenv..."
          pip install pipenv
          echo "[*] Syncing `Pipfile.lock`..."
          pipenv sync --dev
      - name: Run linter
        run: pipenv run black --diff --check src/
      - name: Run unit tests
        run: |
          pipenv run pytest --cov=src tests/unit/
          printf "# Unit test coverage\n\n" >> "$GITHUB_STEP_SUMMARY"
          pipenv run coverage report --skip-empty -m --format=markdown --fail-under "${{vars.TEST_COVERAGE_MINIMUM}}" | tee -a "$GITHUB_STEP_SUMMARY"

          # Stop the pipeline if under test coverage minimum.
          tail -n 1 "$GITHUB_STEP_SUMMARY" | grep -v "Coverage failure: "
      # TODO: re-use artifact if nothing has changed.
      - name: Build artifact
        run: |
          echo "[*] Downloading dependencies..."
          mkdir package/
          pipenv requirements > requirements.txt
          pipenv run pip install -t package/ -r requirements.txt
          echo "[*] Copying source files..."
          ls src
          cp -r src package/
          echo "[*] Bundling artifact..."
          cd package/
          zip -r "../$artifact_filename" .
          cd ..
          echo "[*] Cleaning up workspace..."
          rm -r requirements.txt package/
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{vars.AWS_REGION}}
          role-to-assume: ${{vars.AWS_DEPLOY_ROLE_ARN}}
          role-session-name: ${{needs.init.outputs.pipeline_id}}
      # The next three steps, for two reasons:
      # 1. CFTs can't deploy code changes using a local zipfile.
      # 2. Avoid using Github artifacts/storage quota.
      - name: Store artifact in S3
        run: aws s3 cp "$artifact_filename" "s3://${{vars.S3_ARTIFACT_BUCKET_NAME}}/${{needs.init.outputs.artifact_bucket_key}}"
      - name: Write CFT content to output
        id: write_cft
        run: |
          {
            echo "cft_content<<EOF"
            cat cft.yml
            echo "EOF"
          } | tee -a "$GITHUB_OUTPUT"
      - name: Write env vars to output
        id: write_env_vars
        run: |
          # Use jq to create ENV_VARS w/ env and PR author details.
          export ENV_VARS=$(jq -cn '${{toJSON(vars)}} + {"ENV": "${{needs.init.outputs.deployment_env}}", "GITHUB_ASSIGNEE_IDS": ${{needs.init.outputs.github_assignee_ids}}}')
          python src/config.py --no-pretty
          echo "env_vars=$(cat env.json)" | tee -a "$GITHUB_OUTPUT"
    outputs:
      cft_content: ${{steps.write_cft.outputs.cft_content}}
      env_vars: ${{steps.write_env_vars.outputs.env_vars}}
  deploy:
    name: Deploy
    needs: [init, build]
    runs-on: ubuntu-latest
    environment: ${{needs.init.outputs.deployment_env}}
    env:
      artifact_bucket_key: ${{needs.init.outputs.artifact_bucket_key}}
      deployment_env: ${{needs.init.outputs.deployment_env}}
      repository_name: ${{needs.init.outputs.repository_name}}
    steps:
      # Pull directly from outputs (because of content size).
      - name: Read CFT content from previous output
        run: |
          cat > cft.yml <<"EOL"
          ${{needs.build.outputs.cft_content}}
          EOL
          stat cft.yml
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-region: ${{vars.AWS_REGION}}
          role-to-assume: ${{vars.AWS_DEPLOY_ROLE_ARN}}
          role-session-name: ${{needs.init.outputs.pipeline_id}}
      - name: Deploy to AWS
        uses: aws-actions/aws-cloudformation-github-deploy@v1
        with:
          name: ${{env.repository_name}}-${{env.deployment_env}}
          template: cft.yml
          capabilities: CAPABILITY_NAMED_IAM
          parameter-overrides: >-
            Component="${{env.repository_name}}",
            Env="${{env.deployment_env}}",
            ArtifactBucketName="${{vars.S3_ARTIFACT_BUCKET_NAME}}",
            CodeArtifactBucketKey="${{env.artifact_bucket_key}}",
            EnvVars='${{needs.build.outputs.env_vars}}',
            APIGatewayID="${{vars.AWS_API_GATEWAY_ID}}",
            APIGatewayRoleARN="${{vars.AWS_API_GATEWAY_ROLE_ARN}}"
          tags: '[{"Key": "env", "Value": "${{env.deployment_env}}"}]'

#
#  validate:
#    name: Validate
#    runs-on: ubuntu-latest
#    steps:
#      - name: Run integration tests
#        run: pipenv run behave
